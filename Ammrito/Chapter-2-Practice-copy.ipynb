{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77509ea5-a91d-4013-a3c0-27d9f6d16986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b41ca132-52d6-473f-aa0a-a2f6fc904fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c5fe7-632a-4367-8723-857e979457e4",
   "metadata": {},
   "source": [
    "I have selected a text from the nltk corpuys and given it a name to detect the total words of the file I am running in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ca42f30-8085-4863-9829-c639c4745486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ce230-b781-4a1f-ba24-4e22b04b12bf",
   "metadata": {},
   "source": [
    "In the following cell I applied the 'concordance' that I learned from the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dd18155-48ad-4fff-9fdc-ebf82e552554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 37 matches:\n",
      "er father , was sometimes taken by surprize at his being still able to pity ` \n",
      "hem do the other any good .\" \" You surprize me ! Emma must do Harriet good : a\n",
      "Knightley actually looked red with surprize and displeasure , as he stood up ,\n",
      "r . Elton , and found to his great surprize , that Mr . Elton was actually on \n",
      "d aid .\" Emma saw Mrs . Weston ' s surprize , and felt that it must be great ,\n",
      "father was quite taken up with the surprize of so sudden a journey , and his f\n",
      "y , in all the favouring warmth of surprize and conjecture . She was , moreove\n",
      "he appeared , to have her share of surprize , introduction , and pleasure . Th\n",
      "ir plans ; and it was an agreeable surprize to her , therefore , to perceive t\n",
      "talking aunt had taken me quite by surprize , it must have been the death of m\n",
      "f all the dialogue which ensued of surprize , and inquiry , and congratulation\n",
      " the present . They might chuse to surprize her .\" Mrs . Cole had many to agre\n",
      "the mode of it , the mystery , the surprize , is more like a young woman ' s s\n",
      " to her song took her agreeably by surprize -- a second , slightly but correct\n",
      "\" \" Oh ! no -- there is nothing to surprize one at all .-- A pretty fortune ; \n",
      "t to be considered . Emma ' s only surprize was that Jane Fairfax should accep\n",
      "of your admiration may take you by surprize some day or other .\" Mr . Knightle\n",
      "ation for her will ever take me by surprize .-- I never had a thought of her i\n",
      " expected by the best judges , for surprize -- but there was great joy . Mr . \n",
      " sound of at first , without great surprize . \" So unreasonably early !\" she w\n",
      "d Frank Churchill , with a look of surprize and displeasure .-- \" That is easy\n",
      "; and Emma could imagine with what surprize and mortification she must be retu\n",
      "tled that Jane should go . Quite a surprize to me ! I had not the least idea !\n",
      " . It is impossible to express our surprize . He came to speak to his father o\n",
      "g engaged !\" Emma even jumped with surprize ;-- and , horror - struck , exclai\n"
     ]
    }
   ],
   "source": [
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "emma.concordance('surprize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ecebd-4e91-43c5-86b3-54a645dd748b",
   "metadata": {},
   "source": [
    "I applied another verson of 'import' satatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fb56421-6783-42f9-9b09-a7fe3ad77e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22d89e71-69e6-46b5-bee0-3850e64426b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a942bfe-6f94-459c-aa43-d4a158ca0758",
   "metadata": {},
   "source": [
    "The follwoing code performs a basic text analysis on books from the Gutenberg corpus using the Natural Language Toolkit (NLTK). The goal is to calculate and compare three key statistics for each text: average word length (characters per word), average sentence length (words per sentence), and lexical diversity (vocabulary richness). Average word length measures how long words are on average, giving insight into the complexity of the vocabulary, with shorter words typically being easier to read. Average sentence length tells us how many words are used in a typical sentence, with shorter sentences often being simpler and longer sentences indicating more complex writing structures. Lexical diversity helps us understand how varied the vocabulary of a book is by comparing the total number of words to the number of unique words used in the text. A higher lexical diversity means the author uses a wider range of words, while a lower diversity suggests the same words are repeated more frequently. For each book in the Gutenberg corpus, the code outputs these statistics along with the filename, providing a quick way to compare different writing styles in terms of readability and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d95a1ff-36a3-4f71-9c79-4d50edea0dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars / num_words), round(num_words / num_sents), round(num_words / num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f61de8-19f0-44b4-b8c1-d880536c4f17",
   "metadata": {},
   "source": [
    "The line of code num_vocab = len(set(w.lower() for w in gutenberg.words(fileid))) calculates the number of unique words in a text from the Gutenberg corpus while ignoring case differences. It first retrieves all the words in the text using gutenberg.words(fileid), which gives a list of words, including repetitions. Then, for each word, it converts the word to lowercase using w.lower() to ensure that words like \"The\" and \"the\" are treated as the same word. The set() function is used to remove any duplicate words, as sets only store unique items. Finally, the len() function is applied to the set to count how many unique words there are in total. This process gives the vocabulary size of the text, indicating how many distinct words appear in it. For example, if the text contains words like \"The\", \"cat\", \"sat\", and \"the\" again, the set would count \"the\" only once, resulting in a smaller, more accurate count of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05b0a3d2-a430-47fc-935e-f1a48c0222d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2328716199.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[34], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    macbeth_sentences\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    " macbeth_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180be956-da17-452a-a1bd-e2e333ac9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d164d-0631-428d-ab11-db25517fef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_sentences[1116]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602dcc5-0cbe-42b8-aba4-54ffe71c0441",
   "metadata": {},
   "source": [
    "The following code is designed to find the longest sentence, in terms of word count, from Shakespeare's Macbeth in the Gutenberg corpus. First, the line longest_len = max(len(s) for s in macbeth_sentences) calculates the length of the longest sentence by counting the number of words in each sentence using len(s) and finding the maximum value with the max() function. Next, the list comprehension [s for s in macbeth_sentences if len(s) == longest_len] returns all the sentences that have a length equal to this maximum value. Essentially, this part of the code filters through the sentences in Macbeth and includes only those that match the longest sentence length. The output will be a list of sentences (each represented as a list of words) that have the highest word count. This process helps identify the longest sentence or sentences in the text based on the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfa9bf-d5d3-4700-a117-c56a498a93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "[s for s in macbeth_sentences if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54107c11-aaec-4cdd-bf52-b2be6e3067b3",
   "metadata": {},
   "source": [
    "The following code loops through all the files in the NLTK webtext corpus and prints the name of each file along with a short preview of its content. It uses the webtext.fileids() function to get the list of file names, and then the webtext.raw() function extracts the first 65 characters from each file to provide a brief glimpse of its content. The purpose of this code is to allow users to quickly check the type of content available in each file without needing to load the entire text, which is useful for understanding the data before performing further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52661c86-dc29-4f7a-8c50-692f91a2fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab69e3d-747d-421e-8d65-4f43867d72e2",
   "metadata": {},
   "source": [
    "The following code accesses and inspects a specific post from an online chatroom in the nps_chat corpus of NLTK. First, it imports the corpus and retrieves all posts from a specific chatroom file, '10-19-20s_706posts.xml', which contains 706 messages exchanged by participants aged 10 to 19. The variable chatroom stores these posts as a list, where each post is broken down into individual words. The code then accesses the post at index 123, allowing you to examine a particular message from the chatroom. This is useful for analyzing chatroom data, understanding the structure of informal conversations, or extracting individual posts for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e12fa6-fadc-4741-875d-3bf662a98989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "chatroom[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7be93-7b5d-427e-acd4-29ae113ace6e",
   "metadata": {},
   "source": [
    "The following code accesses the Brown corpus from NLTK and demonstrates how to explore its different categories and retrieve specific text data. First, it lists all available categories (or genres) in the corpus, such as adventure, fiction, and news. Next, the code retrieves all the words from the texts classified under the 'news' category, providing a word-level view of news articles. It also shows how to access the words from a specific file ('cg22') within the corpus, allowing you to work with specific documents. Finally, it demonstrates how to extract sentences from multiple categories, like news, editorials, and reviews, with each sentence represented as a list of words. This functionality is useful for analyzing language use across different text genres and for specific text extraction within the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e4ef1-70e3-477a-b1f4-5be452aeb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71bebb-0e30-4ed9-8e96-886fa92de4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bee2f9-ba78-45de-9caa-468fb36af14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words(fileids=['cg22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c487a13-c283-48db-98a1-10e0eb148397",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents(categories=['news', 'editorial', 'reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753a84d-becd-48e2-bd8c-ae356d0738df",
   "metadata": {},
   "source": [
    "The following code performs a frequency analysis of modal verbs in the news articles from the Brown corpus using the NLTK library. It first retrieves all the words from the news category of the corpus and then creates a frequency distribution of these words, converting them to lowercase to ensure consistent counting. The list of modal verbs includes \"can,\" \"could,\" \"may,\" \"might,\" \"must,\" and \"will.\" For each of these modals, the code prints how frequently it appears in the news text by looking up the count in the frequency distribution. This analysis is useful for understanding how often modal verbs, which express necessity, possibility, or permission, are used in news writing, giving insight into the tone and style of the language used in the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432c001-8dcb-4e86-ae90-1c79bb68de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907af13-8d9b-4426-ad01-19c7b7147383",
   "metadata": {},
   "source": [
    "The following code analyzes how often certain modal verbs (such as \"can,\" \"could,\" \"may,\" \"might,\" \"must,\" and \"will\") appear in different genres of the Brown corpus. It begins by constructing a conditional frequency distribution (CFD) that records the frequency of each word in each genre. The code focuses on six genres: news, religion, hobbies, science fiction, romance, and humor. It then creates a table that shows how frequently the selected modal verbs are used in these genres. This analysis provides insights into how different types of writing, such as news articles or science fiction, use modal verbs, helping to reveal differences in tone, style, and linguistic patterns across genres. The table format makes it easy to compare the use of modal verbs between different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ce84d-08cc-49f9-9574-7e2a4c8076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "(genre, word)\n",
    "for genre in brown.categories()\n",
    "for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae6277-fc6e-4734-afbd-97fb5d95c21d",
   "metadata": {},
   "source": [
    "In the following code, I access the Reuters corpus in NLTK to retrieve the list of file IDs and categories. By using reuters.fileids(), I can get the unique identifiers for individual news articles, which are grouped into either the test or training set. This helps me pinpoint specific documents in the dataset. Additionally, with reuters.categories(), I am able to retrieve the topics under which the articles are classified, providing insight into the various subject areas covered in the corpus. This is helpful when I need to conduct topic-based analysis or classification of news content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52673b48-7222-4c6b-8091-673b0a2edef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecb26c-517b-4ce8-91d3-99ca9719d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef94f6b-8ef4-4945-9768-58c090846445",
   "metadata": {},
   "source": [
    "In the following code, I retrieve specific categories and file IDs from the Reuters corpus in NLTK. First, I use reuters.categories() to find the categories associated with a single document ('training/9865'), and then for multiple documents ('training/9865' and 'training/9880'). This helps me identify the topics under which specific documents are classified. Next, using reuters.fileids(), I retrieve the file IDs for documents that belong to the category 'barley' and then for documents under both 'barley' and 'corn'. This allows me to locate all the relevant articles related to these topics, making it easier to analyze content related to specific subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4350a-c813-4f0a-9d27-d9253f27f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1112313-fd92-4ddf-a801-666a16ae4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21bc16-490c-42a7-acad-f77d7a3aec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.fileids('barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564434a1-8288-415a-8191-037372c8acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.fileids(['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd9927-b632-4f15-a796-978b75833065",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words('training/9865')[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb3144-c23b-4b2a-9c45-b8178ba28556",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9bb74-3317-4032-8042-91b79698b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(categories='barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9261cfc-bf09-4093-9d01-3d25367ab5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(categories=['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea8997-fbb6-4d8f-9dc1-2b66ef04eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in text:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6ebc8-77ee-4430-9b03-caa0159ce552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This\" == \"this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f3070-0f74-42d2-9e9e-fabc83ea7720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
