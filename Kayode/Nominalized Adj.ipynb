{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dd215a-d588-4212-86a0-25a324eae2cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "###Extracting Nominalized Adjectives from TAYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d079c759-cd15-4b02-923a-d6c410bd51c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominalized Adjectives Found:\n",
      "['american', 'unpainted', 'big', 'american', 'average', 'cramped', 'next', 'long', 'last', 'bright', 'other', 'tiny', 'stained', 'small', 'public', 'lumpy', 'american', 'next', 'secondary', 'surprising', 'rich', 'large', 'rich', 'many', 'daily', 'same', 'superior', 'next', 'same', 'third', 'silly', 'fourth', 'only', 'bright', 'only', 'following', 'fifth', 'first', 'next', 'following', 'rust-eaten', 'big', 'other', 'african', 'ghanaian', 'white', 'chinese', 'old', 'calm', 'poor', 'real', 'poor', 'nasty', 'nice', 'old', 'black', 'black', 'black', 'black', 'white', 'big', 'crisp', 'misspelled']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Download NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words and determiners to help identify nominalized adjectives\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "determiners = {\"the\", \"this\", \"that\", \"these\", \"those\"}\n",
    "\n",
    "# Function to extract nominalized adjectives from text\n",
    "def extract_nominalized_adjectives(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    nominalized_adjectives = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        for i in range(len(tagged) - 1):\n",
    "            word, pos = tagged[i]\n",
    "            next_word, next_pos = tagged[i + 1]\n",
    "            \n",
    "            # Check for a determiner followed by an adjective (likely nominalized)\n",
    "            if word in determiners and next_pos == \"JJ\":\n",
    "                nominalized_adjectives.append(next_word)\n",
    "    \n",
    "    return nominalized_adjectives\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:/Users/amusa/code/learning/TXT files/TAYN.txt\"\n",
    "\n",
    "# Ensure the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Extract nominalized adjectives\n",
    "    nominalized_adjectives = extract_nominalized_adjectives(text)\n",
    "    \n",
    "    print(\"Nominalized Adjectives Found:\")\n",
    "    print(nominalized_adjectives)\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d808ceb-1485-4239-a317-d58a2b02460e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominalized Adjectives with Context:\n",
      "Adjective: american, Context: you won the american visa\n",
      "Adjective: unpainted, Context: leaning against the unpainted walls\n",
      "Adjective: big, Context: comparison to the big car\n",
      "Adjective: american, Context: members for the american visa\n",
      "Adjective: average, Context: more than the average salary\n",
      "Adjective: cramped, Context: came into the cramped basement\n",
      "Adjective: next, Context: , and the next morning\n",
      "Adjective: long, Context: , walking the long windy\n",
      "Adjective: last, Context: it was the last stop\n",
      "Adjective: bright, Context: restaurant with the bright ,\n",
      "Adjective: other, Context: less than the other waitresses\n",
      "Adjective: tiny, Context: rent for the tiny room\n",
      "Adjective: stained, Context: room with the stained carpet\n",
      "Adjective: small, Context: besides , the small connecticut\n",
      "Adjective: public, Context: went to the public library\n",
      "Adjective: lumpy, Context: sat on the lumpy mattress\n",
      "Adjective: american, Context: you won the american visa\n",
      "Adjective: next, Context: neighbors from the next room\n",
      "Adjective: secondary, Context: fees at the secondary school\n",
      "Adjective: surprising, Context: write about the surprising openness\n",
      "Adjective: rich, Context: write about the rich people\n",
      "Adjective: large, Context: front of the large compounds\n",
      "Adjective: rich, Context: to write that rich americans\n",
      "Adjective: many, Context: fat and that many did\n",
      "Adjective: daily, Context: you recited the daily specials\n",
      "Adjective: same, Context: little were the same condescending\n",
      "Adjective: superior, Context: head in the superior way\n",
      "Adjective: next, Context: came in the next day\n",
      "Adjective: same, Context: sat at the same table\n",
      "Adjective: third, Context: came in the third day\n",
      "Adjective: silly, Context: any of the silly tourist\n",
      "Adjective: fourth, Context: the fourth day\n",
      "Adjective: only, Context: king was the only maudlin\n",
      "Adjective: bright, Context: him in the bright light\n",
      "Adjective: only, Context: oil was the only thing\n",
      "Adjective: following, Context: said no the following four\n",
      "Adjective: fifth, Context: then , the fifth night\n",
      "Adjective: first, Context: prayed for the first time\n",
      "Adjective: next, Context: the next day\n",
      "Adjective: following, Context: rooted for the following ,\n",
      "Adjective: rust-eaten, Context: because of the rust-eaten hole\n",
      "Adjective: big, Context: the big man\n",
      "Adjective: other, Context: the other cars\n",
      "Adjective: african, Context: he found the african store\n",
      "Adjective: ghanaian, Context: had , the ghanaian store\n",
      "Adjective: white, Context: , like the white kenyans\n",
      "Adjective: chinese, Context: came , the chinese man\n",
      "Adjective: old, Context: added that the old man\n",
      "Adjective: calm, Context: walked along the calm water\n",
      "Adjective: poor, Context: call only the poor indians\n",
      "Adjective: real, Context: in bombay the real indians\n",
      "Adjective: poor, Context: not like the poor fat\n",
      "Adjective: nasty, Context: abnormalâ€”the way the nasty ones\n",
      "Adjective: nice, Context: nasty and the nice ones\n",
      "Adjective: old, Context: the old white\n",
      "Adjective: black, Context: him , the black men\n",
      "Adjective: black, Context: you , the black women\n",
      "Adjective: black, Context: or the black women\n",
      "Adjective: black, Context: smiles ; the black men\n",
      "Adjective: white, Context: him ; the white men\n",
      "Adjective: big, Context: the buildings that big around\n",
      "Adjective: crisp, Context: in between the crisp dollar\n",
      "Adjective: misspelled, Context: , from the misspelled words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Download NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words and determiners to help identify nominalized adjectives\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "determiners = {\"the\", \"this\", \"that\", \"these\", \"those\"}\n",
    "\n",
    "# Function to extract nominalized adjectives with context\n",
    "def extract_nominalized_adjectives_with_context(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    nominalized_adjectives_with_context = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        for i in range(len(tagged) - 1):\n",
    "            word, pos = tagged[i]\n",
    "            next_word, next_pos = tagged[i + 1]\n",
    "            \n",
    "            # Check for a determiner followed by an adjective (likely nominalized)\n",
    "            if word in determiners and next_pos == \"JJ\":\n",
    "                start_index = max(0, i - 2)  # Two words before\n",
    "                end_index = min(len(tokens), i + 3)  # Two words after\n",
    "                context = \" \".join(tokens[start_index:end_index])\n",
    "                nominalized_adjectives_with_context.append((next_word, context))\n",
    "    \n",
    "    return nominalized_adjectives_with_context\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:/Users/amusa/code/learning/TXT files/TAYN.txt\"\n",
    "\n",
    "# Ensure the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Extract nominalized adjectives with context\n",
    "    nominalized_adjectives_with_context = extract_nominalized_adjectives_with_context(text)\n",
    "    \n",
    "    print(\"Nominalized Adjectives with Context:\")\n",
    "    for adjective, context in nominalized_adjectives_with_context:\n",
    "        print(f\"Adjective: {adjective}, Context: {context}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b19efbd-5c32-418f-8fac-beaf96fa2b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominalized Adjectives with Context (No Noun Following):\n",
      "Adjective: average, Context: more than the average salary\n",
      "Adjective: long, Context: , walking the long windy\n",
      "Adjective: bright, Context: restaurant with the bright ,\n",
      "Adjective: many, Context: fat and that many did\n",
      "Adjective: following, Context: said no the following four\n",
      "Adjective: following, Context: rooted for the following ,\n",
      "Adjective: poor, Context: not like the poor fat\n",
      "Adjective: old, Context: the old white\n",
      "Adjective: big, Context: the buildings that big around\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Download NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words and determiners to help identify nominalized adjectives\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "determiners = {\"the\", \"this\", \"that\", \"these\", \"those\"}\n",
    "\n",
    "# Function to extract nominalized adjectives with context\n",
    "def extract_nominalized_adjectives_with_context(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    nominalized_adjectives_with_context = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        for i in range(len(tagged) - 2):  # Ensure there's a word after the adjective\n",
    "            word, pos = tagged[i]\n",
    "            next_word, next_pos = tagged[i + 1]\n",
    "            following_word, following_pos = tagged[i + 2]\n",
    "            \n",
    "            # Check for a determiner followed by an adjective, where the next word is not a noun\n",
    "            if word in determiners and next_pos == \"JJ\" and not following_pos.startswith(\"NN\"):\n",
    "                start_index = max(0, i - 2)  # Two words before\n",
    "                end_index = min(len(tokens), i + 3)  # Two words after\n",
    "                context = \" \".join(tokens[start_index:end_index])\n",
    "                nominalized_adjectives_with_context.append((next_word, context))\n",
    "    \n",
    "    return nominalized_adjectives_with_context\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:/Users/amusa/code/learning/TXT files/TAYN.txt\"\n",
    "\n",
    "# Ensure the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Extract nominalized adjectives with context\n",
    "    nominalized_adjectives_with_context = extract_nominalized_adjectives_with_context(text)\n",
    "    \n",
    "    print(\"Nominalized Adjectives with Context (No Noun Following):\")\n",
    "    for adjective, context in nominalized_adjectives_with_context:\n",
    "        print(f\"Adjective: {adjective}, Context: {context}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12aac1e-d7fe-4899-ad92-808560c94f01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominalized Adjectives with Context (No Noun Following):\n",
      "Adjective: average, Context: thousand more than the average salary plus\n",
      "Adjective: long, Context: left , walking the long windy road\n",
      "Adjective: bright, Context: the restaurant with the bright , clean\n",
      "Adjective: many, Context: were fat and that many did not\n",
      "Adjective: following, Context: you said no the following four days\n",
      "Adjective: following, Context: you rooted for the following , in\n",
      "Adjective: poor, Context: was not like the poor fat people\n",
      "Adjective: old, Context: the old white men\n",
      "Adjective: big, Context: because the buildings that big around your\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Download NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words and determiners to help identify nominalized adjectives\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "determiners = {\"the\", \"this\", \"that\", \"these\", \"those\"}\n",
    "\n",
    "# Function to extract nominalized adjectives with context\n",
    "def extract_nominalized_adjectives_with_context(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    nominalized_adjectives_with_context = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        for i in range(len(tagged) - 2):  # Ensure there's a word after the adjective\n",
    "            word, pos = tagged[i]\n",
    "            next_word, next_pos = tagged[i + 1]\n",
    "            following_word, following_pos = tagged[i + 2]\n",
    "            \n",
    "            # Check for a determiner followed by an adjective, where the next word is not a noun\n",
    "            if word in determiners and next_pos == \"JJ\" and not following_pos.startswith(\"NN\"):\n",
    "                start_index = max(0, i - 3)  # Three words before\n",
    "                end_index = min(len(tokens), i + 4)  # Three words after\n",
    "                context = \" \".join(tokens[start_index:end_index])\n",
    "                nominalized_adjectives_with_context.append((next_word, context))\n",
    "    \n",
    "    return nominalized_adjectives_with_context\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:\\Users\\amusa\\code\\learning\\TXT files\\TAYN.txt\"\n",
    "\n",
    "# Ensure the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Extract nominalized adjectives with context\n",
    "    nominalized_adjectives_with_context = extract_nominalized_adjectives_with_context(text)\n",
    "    \n",
    "    print(\"Nominalized Adjectives with Context (No Noun Following):\")\n",
    "    for adjective, context in nominalized_adjectives_with_context:\n",
    "        print(f\"Adjective: {adjective}, Context: {context}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e621d012-babf-4260-8352-1de9b9aaba0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives with Context (Preceded by 'The' Only):\n",
      "Adjective: average, Context: thousand more than the average salary plus\n",
      "Adjective: long, Context: left , walking the long windy road\n",
      "Adjective: bright, Context: the restaurant with the bright , clean\n",
      "Adjective: following, Context: you said no the following four days\n",
      "Adjective: following, Context: you rooted for the following , in\n",
      "Adjective: poor, Context: was not like the poor fat people\n",
      "Adjective: old, Context: the old white men\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Download NLTK data files (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to extract adjectives preceded only by \"the\" and include their context\n",
    "def extract_adjectives_with_the_context(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    adjectives_with_context = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        for i in range(len(tagged) - 2):  # Ensure there's a word after the adjective\n",
    "            word, pos = tagged[i]\n",
    "            next_word, next_pos = tagged[i + 1]\n",
    "            following_word, following_pos = tagged[i + 2]\n",
    "            \n",
    "            # Check for \"the\" followed by an adjective, where the next word is not a noun\n",
    "            if word == \"the\" and next_pos == \"JJ\" and not following_pos.startswith(\"NN\"):\n",
    "                start_index = max(0, i - 3)  # Three words before \"the\"\n",
    "                end_index = min(len(tokens), i + 4)  # Three words after the adjective\n",
    "                context = \" \".join(tokens[start_index:end_index])\n",
    "                adjectives_with_context.append((next_word, context))\n",
    "    \n",
    "    return adjectives_with_context\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:/Users/amusa/code/learning/TXT files/TAYN.txt\"\n",
    "\n",
    "# Ensure the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Extract adjectives with \"the\" and context\n",
    "    adjectives_with_context = extract_adjectives_with_the_context(text)\n",
    "    \n",
    "    print(\"Adjectives with Context (Preceded by 'The' Only):\")\n",
    "    for adjective, context in adjectives_with_context:\n",
    "        print(f\"Adjective: {adjective}, Context: {context}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d9c66-af84-4534-b377-fa20161a5d78",
   "metadata": {},
   "source": [
    "####Nominalized Adjectives Using Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d505da-0f93-45e2-9770-88c598d6e98e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives with Context (Preceded by 'The' Only):\n",
      "Adjective: NECK, Context: they trooped into the room in lagos\n",
      "Adjective: NECK, Context: in comparison to the big car and\n",
      "Adjective: NECK, Context: you walked into the restaurant with the\n",
      "Adjective: NECK, Context: many people at the restaurant asked when\n",
      "Adjective: NECK, Context: he came in the next day and\n",
      "Adjective: NECK, Context: he came in the third day and\n",
      "Adjective: NECK, Context: you said no the following four days\n",
      "Adjective: NECK, Context: and then , the fifth night ,\n",
      "Adjective: NECK, Context: you prayed for the first time in\n",
      "Adjective: NECK, Context: the rain , the swampiness , you\n",
      "Adjective: NECK, Context: back home , the meat pieces you\n",
      "Adjective: NECK, Context: later , in the shower , you\n",
      "Adjective: NECK, Context: your mother wrote the letter herself ;\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract adjectives preceded by \"the\" and their context\n",
    "def extract_adjectives_with_the_context_spacy(text):\n",
    "    nominalized_adjectives_with_context = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text.lower() for token in sent]  # Tokenize the sentence\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Check if the current token is \"the\" and is followed by an adjective\n",
    "            if token == \"the\" and i + 1 < len(tokens):\n",
    "                next_token = doc[i + 1]\n",
    "                if next_token.pos_ == \"ADJ\":  # Ensure next token is an adjective\n",
    "                    # Ensure the word after the adjective is NOT a noun\n",
    "                    if i + 2 < len(tokens) and doc[i + 2].pos_ != \"NOUN\":\n",
    "                        # Extract three words before and after for context\n",
    "                        start_index = max(0, i - 3)  # Three words before \"the\"\n",
    "                        end_index = min(len(tokens), i + 4)  # Three words after the adjective\n",
    "                        context = \" \".join(tokens[start_index:end_index])\n",
    "                        nominalized_adjectives_with_context.append((next_token.text, context))\n",
    "    \n",
    "    return nominalized_adjectives_with_context\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:/Users/amusa/code/learning/TXT files/TAYN.txt\"\n",
    "\n",
    "# Ensure the file exists\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Extract nominalized adjectives with spaCy\n",
    "    nominalized_adjectives_with_context = extract_adjectives_with_the_context_spacy(text)\n",
    "    \n",
    "    print(\"Adjectives with Context (Preceded by 'The' Only):\")\n",
    "    for adjective, context in nominalized_adjectives_with_context:\n",
    "        print(f\"Adjective: {adjective}, Context: {context}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8e29e-61fc-44ae-ab78-4a5b3f3e99a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
